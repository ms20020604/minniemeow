import org.opencv.core.*;
import org.opencv.imgcodecs.Imgcodecs;
import org.opencv.imgproc.Imgproc;
import org.opencv.objdetect.CascadeClassifier;
import org.tensorflow.lite.Interpreter;

import javax.swing.*;
import java.awt.*;
import java.awt.image.BufferedImage;
import java.io.File;
import java.io.IOException;
import java.nio.MappedByteBuffer;
import java.nio.channels.FileChannel;
import java.util.*;
import java.util.List;
import javax.imageio.ImageIO;
import java.io.ByteArrayInputStream;

public class MoodMirror {

    private static Interpreter interpreter;

    public static void main(String[] args) {
        // Load OpenCV library
        System.loadLibrary(Core.NATIVE_LIBRARY_NAME);

        // Load face detection model
        String faceCascadePath = "haarcascade_frontalface_default.xml";
        CascadeClassifier faceDetector = new CascadeClassifier(faceCascadePath);

        // Load the TensorFlow Lite model
        try {
            interpreter = new Interpreter(loadModelFile("path/to/emotion_model.tflite"));
        } catch (IOException e) {
            System.err.println("Error loading TensorFlow Lite model: " + e.getMessage());
            return;
        }

        // Load the camera feed
        VideoCapture camera = new VideoCapture(0);
        if (!camera.isOpened()) {
            System.out.println("Error: Camera not found!");
            return;
        }

        Mat frame = new Mat();
        JFrame displayWindow = createDisplayWindow("AI Mood Mirror");

        while (true) {
            // Read a frame from the camera
            camera.read(frame);
            if (frame.empty()) {
                continue;
            }

            // Detect faces in the frame
            MatOfRect faces = new MatOfRect();
            faceDetector.detectMultiScale(frame, faces);

            // Draw rectangles around detected faces and predict emotions
            for (Rect rect : faces.toArray()) {
                Imgproc.rectangle(frame, rect.tl(), rect.br(), new Scalar(0, 255, 0), 3);
                Mat faceROI = frame.submat(rect);

                // Preprocess faceROI for emotion model
                Mat resizedFace = preprocessFace(faceROI);

                // Predict emotion using the AI model
                String emotion = predictEmotion(resizedFace);

                // Display emotion label
                Imgproc.putText(frame, emotion, new Point(rect.x, rect.y - 10),
                        Imgproc.FONT_HERSHEY_SIMPLEX, 1.0, new Scalar(255, 255, 255), 2);
            }

            // Display the frame
            updateDisplay(displayWindow, frame);

            // Exit on pressing 'q'
            if (new Scanner(System.in).hasNext("q")) {
                break;
            }
        }

        camera.release();
        interpreter.close(); // Close the TensorFlow Lite interpreter
    }

    private static JFrame createDisplayWindow(String title) {
        JFrame frame = new JFrame(title);
        frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);
        frame.setSize(800, 600);
        frame.setVisible(true);
        return frame;
    }

    private static void updateDisplay(JFrame frame, Mat mat) {
        BufferedImage image = convertMatToBufferedImage(mat);
        ImageIcon imageIcon = new ImageIcon(image);
        JLabel label = new JLabel(imageIcon);
        frame.setContentPane(label);
        frame.revalidate();
    }

    private static BufferedImage convertMatToBufferedImage(Mat mat) {
        MatOfByte mob = new MatOfByte();
        Imgcodecs.imencode(".jpg", mat, mob);
        byte[] byteArray = mob.toArray();
        ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(byteArray);
        try {
            return ImageIO.read(byteArrayInputStream);
        } catch (IOException e) {
            e.printStackTrace();
            return null;
        }
    }

    private static Mat preprocessFace(Mat face) {
        Mat gray = new Mat();
        Imgproc.cvtColor(face, gray, Imgproc.COLOR_BGR2GRAY);
        Mat resized = new Mat();
        Imgproc.resize(gray, resized, new Size(48, 48)); // Assuming 48x48 input for emotion model
        return resized;
    }

    private static String predictEmotion(Mat face) {
        // Preprocess face and convert it to a tensor for the model
        float[][][][] inputTensor = preprocessFaceForTensor(face);
        float[][] outputBuffer = new float[1][6]; // Assuming 6 possible emotions

        interpreter.run(inputTensor, outputBuffer);

        // Get the index of the maximum value from outputBuffer (predicted emotion)
        int predictedIndex = getMaxIndex(outputBuffer[0]);

        // List of emotions (adjust as necessary)
        List<String> emotions = Arrays.asList("Happy", "Sad", "Angry", "Surprised", "Fear", "Neutral");
        return emotions.get(predictedIndex);
    }

    private static float[][][][] preprocessFaceForTensor(Mat face) {
        // Convert the face image (48x48 grayscale) to a float tensor
        float[][][][] tensor = new float[1][48][48][1];
        for (int i = 0; i < 48; i++) {
            for (int j = 0; j < 48; j++) {
                tensor[0][i][j][0] = face.get(i, j)[0] / 255.0f; // Normalize the pixel value
            }
        }
        return tensor;
    }

    private static int getMaxIndex(float[] array) {
        int index = 0;
        float max = array[0];
        for (int i = 1; i < array.length; i++) {
            if (array[i] > max) {
                max = array[i];
                index = i;
            }
        }
        return index;
    }

    private static MappedByteBuffer loadModelFile(String modelPath) throws IOException {
        File file = new File(modelPath);
        FileInputStream fileInputStream = new FileInputStream(file);
        FileChannel fileChannel = fileInputStream.getChannel();
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, 0, file.length());
    }
}
